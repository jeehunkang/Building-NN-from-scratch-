{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "#### (Building own NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow \n",
    "\n",
    "\n",
    "\n",
    "x_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.train.npy')\n",
    "submission = pd.read_csv('/kaggle/input/cisc6000-deep-learning-hw1/sample_submission.csv')\n",
    "y_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.trainlabel.npy')\n",
    "x_test = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.test.npy')\n",
    "\n",
    "def seed_everything(SEED): \n",
    "    np.random.seed(SEED) \n",
    "    random.seed(SEED)\n",
    "\n",
    "seed_everything(2425)\n",
    "\n",
    "# prepare training (80%) and validation data (20%)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.2)\n",
    "\n",
    "# data reshape\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "#y_train = to_categorical(y_train)\n",
    "\n",
    "\n",
    "shape = (y_train.size, y_train.max()+1)\n",
    "one_hot = np.zeros(shape)\n",
    "\n",
    "rows = np.arange(y_train.size)\n",
    "one_hot[rows, y_train] = 1\n",
    "y_train = one_hot\n",
    "\n",
    "\n",
    "\n",
    "shape = (y_validation.size, y_validation.max()+1)\n",
    "one_hot = np.zeros(shape)\n",
    "\n",
    "rows = np.arange(y_validation.size)\n",
    "one_hot[rows, y_validation] = 1\n",
    "y_validation = one_hot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Activation functions \n",
    "def tanh(x,derivative = False):\n",
    "    if derivative:\n",
    "        return 1.0 - np.tanh(x)**2  \n",
    "    return np.tanh(x)\n",
    "\n",
    "# Softmax function \n",
    "def softmax(x, derivative = False):\n",
    "    exps = np.exp(x - x.max())\n",
    "    if derivative:\n",
    "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))    \n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NN:\n",
    "    \n",
    "\n",
    "    def __init__(self, input_neurons, output_neurons_first, output_neurons_second, last_neurons):\n",
    "        self.input_neurons = input_neurons\n",
    "        self.output_neurons_first = output_neurons_first\n",
    "        self.output_neurons_second = output_neurons_second\n",
    "        self.last_neurons = last_neurons\n",
    "            \n",
    "        self.w1 = np.random.rand(self.input_neurons, self.output_neurons_first)\n",
    "        self.w2 = np.random.rand(self.output_neurons_first, self.output_neurons_second)\n",
    "        self.w3 = np.random.rand(self.output_neurons_second,self.last_neurons)\n",
    "                                \n",
    "            \n",
    "\n",
    "\n",
    "    def forward_propagation(self,x_train):\n",
    "        z1 = np.dot(x_train, self.w1)\n",
    "        a1 = tanh(z1)\n",
    "        # hiddden 1 > hidden 2\n",
    "        z2 = np.dot(a1, self.w2)\n",
    "        a2 = tanh(z2)\n",
    "        # hidden 2 > output\n",
    "        z3 = np.dot(a2, self.w3)\n",
    "        a3 = softmax(z3)\n",
    "        \n",
    "        return a3 \n",
    "\n",
    "\n",
    "    def backward_propagation(self, y_train, output):\n",
    "        \n",
    "        updated_weight = {}\n",
    "\n",
    "        # w3 update\n",
    "        error = output - y_train \n",
    "        updated_weight['w3'] = np.dot(error, a3)\n",
    "\n",
    "        # w2 update\n",
    "        error = np.multiply(np.dot(self.w3.T, error), self.tanh(z2, derivative = True))\n",
    "        updated_weight['w2'] = np.dot(error, a2)\n",
    "\n",
    "        # w1 update \n",
    "\n",
    "        error = np.multiply(np.dot(self.w2.T, error), self.tanh(z1, derivative = True))\n",
    "        updated_weight['w1'] = np.dot(error, a1)\n",
    "\n",
    "\n",
    "        return updated_weight \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def update_network_parameters(self, updated_weight):\n",
    "\n",
    "        for key, value in updated_weight.items():\n",
    "            for w_arr in updated_weight:\n",
    "                w_arr -= self.l_rate * value\n",
    "                \n",
    "                \n",
    "    def compute_accuracy(self, x_validaton, y_validaton):\n",
    "      \n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(x_validaton, y_validaton):\n",
    "            output = self.forward_propagation(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == y)\n",
    "            \n",
    "            \n",
    "    def train(self, x_train, y_train, x_validaton, y_validaton):\n",
    "        start_time = time.time()\n",
    "        for iteration in range(50):\n",
    "            for x,y in zip(x_train, y_train):\n",
    "                output = self.forward_propagation(x)\n",
    "                updated_weight = self.backward_propagation(y, output)\n",
    "                self.update_network_parameters(updated_weight)\n",
    "\n",
    "            accuracy = self.compute_accuracy(x_validaton, x_validaton)\n",
    "\n",
    "\n",
    "dnn = NN(784, 512, 256, 10)\n",
    "dnn.train(x_train, y_train, x_validation, y_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building NN using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T03:22:02.900417Z",
     "iopub.status.busy": "2022-10-11T03:22:02.900094Z",
     "iopub.status.idle": "2022-10-11T03:24:38.287129Z",
     "shell.execute_reply": "2022-10-11T03:24:38.286364Z",
     "shell.execute_reply.started": "2022-10-11T03:22:02.900336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44800, 28, 28)\n",
      "(11200, 28, 28)\n",
      "(44800,)\n",
      "(11200,)\n",
      "(44800, 28, 28, 1)\n",
      "(11200, 28, 28, 1)\n",
      "x shape\n",
      "(44800, 28, 28, 1)\n",
      "(11200, 28, 28, 1)\n",
      "(28000, 28, 28)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "y_train unique values unpack\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([4458, 5096, 4483, 4574, 4350, 4024, 4340, 4724, 4350, 4401]))\n",
      "y_validation unique values unpack\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([1128, 1239, 1068, 1129, 1069, 1037, 1126, 1158, 1127, 1119]))\n",
      "----------------------------------------------------------------------------------------------------\n",
      "y shape\n",
      "before one hot encoding: (44800,)\n",
      "one hot encoding in process......................\n",
      "(44800, 10)\n",
      "after one hot ecndoing (44800, 10)\n",
      "(44800, 28, 28, 1)\n",
      "(11200, 28, 28, 1)\n",
      "(44800, 10)\n",
      "(11200, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 03:22:11.727484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:11.823608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:11.824500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:11.826122: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-11 03:22:11.827353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:11.828059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:11.828683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:14.087997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:14.088863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:14.089538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-11 03:22:14.090134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2022-10-11 03:22:15.090987: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1400/1400 - 4s - loss: 0.4639 - accuracy: 0.8627\n",
      "Epoch 2/50\n",
      "1400/1400 - 3s - loss: 0.4828 - accuracy: 0.8576\n",
      "Epoch 3/50\n",
      "1400/1400 - 2s - loss: 0.5083 - accuracy: 0.8558\n",
      "Epoch 4/50\n",
      "1400/1400 - 2s - loss: 0.5121 - accuracy: 0.8538\n",
      "Epoch 5/50\n",
      "1400/1400 - 2s - loss: 0.5310 - accuracy: 0.8499\n",
      "Epoch 6/50\n",
      "1400/1400 - 2s - loss: 0.5487 - accuracy: 0.8491\n",
      "Epoch 7/50\n",
      "1400/1400 - 2s - loss: 0.5191 - accuracy: 0.8549\n",
      "Epoch 8/50\n",
      "1400/1400 - 2s - loss: 0.5148 - accuracy: 0.8593\n",
      "Epoch 9/50\n",
      "1400/1400 - 2s - loss: 0.5245 - accuracy: 0.8565\n",
      "Epoch 10/50\n",
      "1400/1400 - 2s - loss: 0.4897 - accuracy: 0.8694\n",
      "Epoch 11/50\n",
      "1400/1400 - 2s - loss: 0.4845 - accuracy: 0.8697\n",
      "Epoch 12/50\n",
      "1400/1400 - 2s - loss: 0.4734 - accuracy: 0.8740\n",
      "Epoch 13/50\n",
      "1400/1400 - 2s - loss: 0.4422 - accuracy: 0.8849\n",
      "Epoch 14/50\n",
      "1400/1400 - 2s - loss: 0.4647 - accuracy: 0.8775\n",
      "Epoch 15/50\n",
      "1400/1400 - 2s - loss: 0.4720 - accuracy: 0.8761\n",
      "Epoch 16/50\n",
      "1400/1400 - 3s - loss: 0.4996 - accuracy: 0.8651\n",
      "Epoch 17/50\n",
      "1400/1400 - 2s - loss: 0.4876 - accuracy: 0.8665\n",
      "Epoch 18/50\n",
      "1400/1400 - 2s - loss: 0.4747 - accuracy: 0.8760\n",
      "Epoch 19/50\n",
      "1400/1400 - 2s - loss: 0.4824 - accuracy: 0.8738\n",
      "Epoch 20/50\n",
      "1400/1400 - 2s - loss: 0.4510 - accuracy: 0.8842\n",
      "Epoch 21/50\n",
      "1400/1400 - 2s - loss: 0.4452 - accuracy: 0.8840\n",
      "Epoch 22/50\n",
      "1400/1400 - 2s - loss: 0.4455 - accuracy: 0.8831\n",
      "Epoch 23/50\n",
      "1400/1400 - 2s - loss: 0.4530 - accuracy: 0.8815\n",
      "Epoch 24/50\n",
      "1400/1400 - 2s - loss: 0.4552 - accuracy: 0.8802\n",
      "Epoch 25/50\n",
      "1400/1400 - 2s - loss: 0.4604 - accuracy: 0.8805\n",
      "Epoch 26/50\n",
      "1400/1400 - 2s - loss: 0.4146 - accuracy: 0.8910\n",
      "Epoch 27/50\n",
      "1400/1400 - 2s - loss: 0.4401 - accuracy: 0.8858\n",
      "Epoch 28/50\n",
      "1400/1400 - 2s - loss: 0.4284 - accuracy: 0.8893\n",
      "Epoch 29/50\n",
      "1400/1400 - 2s - loss: 0.4124 - accuracy: 0.8961\n",
      "Epoch 30/50\n",
      "1400/1400 - 2s - loss: 0.4207 - accuracy: 0.8913\n",
      "Epoch 31/50\n",
      "1400/1400 - 3s - loss: 0.4426 - accuracy: 0.8890\n",
      "Epoch 32/50\n",
      "1400/1400 - 2s - loss: 0.4306 - accuracy: 0.8880\n",
      "Epoch 33/50\n",
      "1400/1400 - 2s - loss: 0.4367 - accuracy: 0.8867\n",
      "Epoch 34/50\n",
      "1400/1400 - 2s - loss: 0.4447 - accuracy: 0.8874\n",
      "Epoch 35/50\n",
      "1400/1400 - 2s - loss: 0.4155 - accuracy: 0.8945\n",
      "Epoch 36/50\n",
      "1400/1400 - 2s - loss: 0.4099 - accuracy: 0.8952\n",
      "Epoch 37/50\n",
      "1400/1400 - 2s - loss: 0.4328 - accuracy: 0.8882\n",
      "Epoch 38/50\n",
      "1400/1400 - 2s - loss: 0.3941 - accuracy: 0.8989\n",
      "Epoch 39/50\n",
      "1400/1400 - 2s - loss: 0.4107 - accuracy: 0.8961\n",
      "Epoch 40/50\n",
      "1400/1400 - 2s - loss: 0.4111 - accuracy: 0.8942\n",
      "Epoch 41/50\n",
      "1400/1400 - 2s - loss: 0.4038 - accuracy: 0.8948\n",
      "Epoch 42/50\n",
      "1400/1400 - 2s - loss: 0.4044 - accuracy: 0.8956\n",
      "Epoch 43/50\n",
      "1400/1400 - 2s - loss: 0.4058 - accuracy: 0.8944\n",
      "Epoch 44/50\n",
      "1400/1400 - 2s - loss: 0.4195 - accuracy: 0.8919\n",
      "Epoch 45/50\n",
      "1400/1400 - 2s - loss: 0.4388 - accuracy: 0.8856\n",
      "Epoch 46/50\n",
      "1400/1400 - 3s - loss: 0.4087 - accuracy: 0.8968\n",
      "Epoch 47/50\n",
      "1400/1400 - 2s - loss: 0.4002 - accuracy: 0.8974\n",
      "Epoch 48/50\n",
      "1400/1400 - 2s - loss: 0.4265 - accuracy: 0.8969\n",
      "Epoch 49/50\n",
      "1400/1400 - 2s - loss: 0.3815 - accuracy: 0.9068\n",
      "Epoch 50/50\n",
      "1400/1400 - 2s - loss: 0.4128 - accuracy: 0.8938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 03:24:37.685618: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at ./h1_result_RMSprop_augmented \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "x_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.train.npy')\n",
    "submission = pd.read_csv('/kaggle/input/cisc6000-deep-learning-hw1/sample_submission.csv')\n",
    "y_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.trainlabel.npy')\n",
    "x_test = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.test.npy')\n",
    "\n",
    "def seed_everything(SEED): \n",
    "    np.random.seed(SEED) \n",
    "    random.seed(SEED)\n",
    "\n",
    "seed_everything(2425)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.2)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "\n",
    "\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
    "print(x_train.shape)\n",
    "\n",
    "x_validation = np.reshape(x_validation, [-1, 28, 28, 1])\n",
    "print(x_validation.shape)\n",
    "\n",
    "\n",
    "\n",
    "#x_train = x_train.reshape(44800, 784)\n",
    "#x_validation = x_validation.reshape(11200, 784)\n",
    "#x_train= x_train.astype('float32')\n",
    "#x_validation = x_validation.astype('float32')\n",
    "#x_test = x_test.reshape(28000, 784)\n",
    "\n",
    "\n",
    "\n",
    "# normalize data for trainining-purpose \n",
    "\n",
    "\n",
    "\n",
    "print('x shape')\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(\"-\"*100)\n",
    "print(\"y_train unique values unpack\")\n",
    "print(np.unique(y_train, return_counts = True))\n",
    "print(\"y_validation unique values unpack\")\n",
    "print(np.unique(y_validation, return_counts = True))\n",
    "print(\"-\"*100)\n",
    "\n",
    "\n",
    "# One hot encoding\n",
    "print('y shape')\n",
    "num_class = 10 \n",
    "print('before one hot encoding:', y_train.shape)\n",
    "print('one hot encoding in process......................')\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_validation = np_utils.to_categorical(y_validation)\n",
    "\n",
    "print(y_train.shape)\n",
    "print('after one hot ecndoing', y_train.shape)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "\n",
    "x_train= x_train.astype('float32')\n",
    "x_validation = x_validation.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#x_train /= 255 \n",
    "#x_validation /= 255\n",
    "#x_test /= 255 \n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (28,28,1)))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('tanh'))\n",
    "    \n",
    "model.add(Dense(256))\n",
    "model.add(Activation('tanh'))\n",
    "    \n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "    \n",
    "model = model \n",
    "\n",
    "    \n",
    "# Compile model \n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = opt)\n",
    "\n",
    "\n",
    "#### saving metrics in history\n",
    "history = model.fit(x_train, y_train, batch_size = 32, epochs= 50, verbose = 2)\n",
    "\n",
    "# model save \n",
    "\n",
    "save_dir = \"./\"\n",
    "model_name = 'h1_result_RMSprop_augmented'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "x_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.train.npy')\n",
    "submission = pd.read_csv('/kaggle/input/cisc6000-deep-learning-hw1/sample_submission.csv')\n",
    "y_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.trainlabel.npy')\n",
    "x_test = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.test.npy')\n",
    "\n",
    "def seed_everything(SEED): \n",
    "    np.random.seed(SEED) \n",
    "    random.seed(SEED)\n",
    "\n",
    "seed_everything(2425)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.2)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "\n",
    "\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
    "print(x_train.shape)\n",
    "\n",
    "x_validation = np.reshape(x_validation, [-1, 28, 28, 1])\n",
    "print(x_validation.shape)\n",
    "\n",
    "\n",
    "\n",
    "#x_train = x_train.reshape(44800, 784)\n",
    "#x_validation = x_validation.reshape(11200, 784)\n",
    "#x_train= x_train.astype('float32')\n",
    "#x_validation = x_validation.astype('float32')\n",
    "#x_test = x_test.reshape(28000, 784)\n",
    "\n",
    "\n",
    "\n",
    "# normalize data for trainining-purpose \n",
    "\n",
    "\n",
    "\n",
    "print('x shape')\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(\"-\"*100)\n",
    "print(\"y_train unique values unpack\")\n",
    "print(np.unique(y_train, return_counts = True))\n",
    "print(\"y_validation unique values unpack\")\n",
    "print(np.unique(y_validation, return_counts = True))\n",
    "print(\"-\"*100)\n",
    "\n",
    "\n",
    "# One hot encoding\n",
    "print('y shape')\n",
    "num_class = 10 \n",
    "print('before one hot encoding:', y_train.shape)\n",
    "print('one hot encoding in process......................')\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_validation = np_utils.to_categorical(y_validation)\n",
    "\n",
    "print(y_train.shape)\n",
    "print('after one hot ecndoing', y_train.shape)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "\n",
    "x_train= x_train.astype('float32')\n",
    "x_validation = x_validation.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#x_train /= 255 \n",
    "#x_validation /= 255\n",
    "#x_test /= 255 \n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (28,28,1)))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('tanh'))\n",
    "    \n",
    "model.add(Dense(256))\n",
    "model.add(Activation('tanh'))\n",
    "    \n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "    \n",
    "model = model \n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = opt)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(        \n",
    "        rotation_range = 45,\n",
    "        horizontal_flip = True,vertical_flip = True \n",
    "        )\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size=32, shuffle=True)\n",
    "validation_generator = datagen.flow(x_validation, y_validation, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "history_inception_datagen = model.fit_generator(train_generator,\n",
    "                                                steps_per_epoch = 1400,\n",
    "                                                epochs=50,\n",
    "                                                validation_data = validation_generator\n",
    "                                                )\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time_fl = (time.time() - start)    \n",
    "print(elapsed_time_fl)\n",
    "\n",
    "\n",
    "save_dir = \"./\"\n",
    "model_name = 'h1_result_RMSprop_augmented'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall validation accuracy & Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation = np_utils.to_categorical(y_validation)\n",
    "\n",
    "mnist_model = load_model(model_name)\n",
    "loss_and_metrics = mnist_model.evaluate(x_validation, y_validation, verbose = 2)\n",
    "\n",
    "print(\"val loss\", loss_and_metrics[0])\n",
    "print(\"val accuracy\", loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each class' accuracy\n",
    "\n",
    "\n",
    "mnist_model = load_model(model_name)\n",
    "y_pred = np.argmax(mnist_model.predict(x_validation), axis = 1)\n",
    "\n",
    "num_correct_class = [0 for i in range(10)]\n",
    "num_sample_class = [0 for i in range(10)]\n",
    "\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    label = y_validation[i]\n",
    "    if y_pred[i] == label:\n",
    "        num_correct_class[label] +=1\n",
    "        \n",
    "    num_sample_class[label]+=1 \n",
    "\n",
    "    \n",
    "for i in range(10):\n",
    "    acc = 100 * num_correct_class[i]/ num_sample_class[i]\n",
    "    print(f' accuray of class {i} = {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data accuracy\n",
    "\n",
    "pred_test = np.argmax(mnist_model.predict(x_test), axis = 1)\n",
    "submission = pd.read_csv('/kaggle/input/cisc6000-deep-learning-hw1/sample_submission.csv',index_col = 0)\n",
    "submission['class'] = pred_test\n",
    "submission.to_csv('RMSprop_augmented-Neural_Network_Submission.csv')\n",
    "submission.head()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
