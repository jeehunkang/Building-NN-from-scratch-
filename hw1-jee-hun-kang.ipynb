{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Question 2 (Building own NN)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport math\nfrom sklearn.model_selection import train_test_split\nimport time\nimport tensorflow \n\n\n\nx_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.train.npy')\nsubmission = pd.read_csv('/kaggle/input/cisc6000-deep-learning-hw1/sample_submission.csv')\ny_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.trainlabel.npy')\nx_test = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.test.npy')\n\ndef seed_everything(SEED): \n    np.random.seed(SEED) \n    random.seed(SEED)\n\nseed_everything(2425)\n\n# prepare training (80%) and validation data (20%)\nx_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.2)\n\n# data reshape\nx_train = x_train.reshape(x_train.shape[0], 784)\n#y_train = to_categorical(y_train)\n\n\nshape = (y_train.size, y_train.max()+1)\none_hot = np.zeros(shape)\n\nrows = np.arange(y_train.size)\none_hot[rows, y_train] = 1\ny_train = one_hot\n\n\n\nshape = (y_validation.size, y_validation.max()+1)\none_hot = np.zeros(shape)\n\nrows = np.arange(y_validation.size)\none_hot[rows, y_validation] = 1\ny_validation = one_hot\n\n\n\n\n    \n# Activation functions \ndef tanh(x,derivative = False):\n    if derivative:\n        return 1.0 - np.tanh(x)**2  \n    return np.tanh(x)\n\n# Softmax function \ndef softmax(x, derivative = False):\n    exps = np.exp(x - x.max())\n    if derivative:\n        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))    \n    return exps / np.sum(exps, axis=0)\n\n\n\n\nclass NN:\n    \n\n    def __init__(self, input_neurons, output_neurons_first, output_neurons_second, last_neurons):\n        self.input_neurons = input_neurons\n        self.output_neurons_first = output_neurons_first\n        self.output_neurons_second = output_neurons_second\n        self.last_neurons = last_neurons\n            \n        self.w1 = np.random.rand(self.input_neurons, self.output_neurons_first)\n        self.w2 = np.random.rand(self.output_neurons_first, self.output_neurons_second)\n        self.w3 = np.random.rand(self.output_neurons_second,self.last_neurons)\n                                \n            \n\n\n    def forward_propagation(self,x_train):\n        z1 = np.dot(x_train, self.w1)\n        a1 = tanh(z1)\n        # hiddden 1 > hidden 2\n        z2 = np.dot(a1, self.w2)\n        a2 = tanh(z2)\n        # hidden 2 > output\n        z3 = np.dot(a2, self.w3)\n        a3 = softmax(z3)\n        \n        return a3 \n\n\n    def backward_propagation(self, y_train, output):\n        \n        updated_weight = {}\n\n        # w3 update\n        error = output - y_train \n        updated_weight['w3'] = np.dot(error, a3)\n\n        # w2 update\n        error = np.multiply(np.dot(self.w3.T, error), self.tanh(z2, derivative = True))\n        updated_weight['w2'] = np.dot(error, a2)\n\n        # w1 update \n\n        error = np.multiply(np.dot(self.w2.T, error), self.tanh(z1, derivative = True))\n        updated_weight['w1'] = np.dot(error, a1)\n\n\n        return updated_weight \n    \n    \n    \n    \n    def update_network_parameters(self, updated_weight):\n\n        for key, value in updated_weight.items():\n            for w_arr in updated_weight:\n                w_arr -= self.l_rate * value\n                \n                \n    def compute_accuracy(self, x_validaton, y_validaton):\n      \n        predictions = []\n\n        for x, y in zip(x_validaton, y_validaton):\n            output = self.forward_propagation(x)\n            pred = np.argmax(output)\n            predictions.append(pred == y)\n            \n            \n    def train(self, x_train, y_train, x_validaton, y_validaton):\n        start_time = time.time()\n        for iteration in range(50):\n            for x,y in zip(x_train, y_train):\n                output = self.forward_propagation(x)\n                updated_weight = self.backward_propagation(y, output)\n                self.update_network_parameters(updated_weight)\n\n            accuracy = self.compute_accuracy(x_validaton, x_validaton)\n\n\ndnn = NN(784, 512, 256, 10)\ndnn.train(x_train, y_train, x_validation, y_validation)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Question 3","metadata":{}},{"cell_type":"markdown","source":"#### Without Augmentation\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport math\nimport random\nimport os\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom keras.models import Sequential, load_model\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.utils import np_utils\n\nx_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.train.npy')\nsubmission = pd.read_csv('/kaggle/input/cisc6000-deep-learning-hw1/sample_submission.csv')\ny_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.trainlabel.npy')\nx_test = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.test.npy')\n\ndef seed_everything(SEED): \n    np.random.seed(SEED) \n    random.seed(SEED)\n\nseed_everything(2425)\n\nx_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.2)\n\nprint(x_train.shape)\nprint(x_validation.shape)\nprint(y_train.shape)\nprint(y_validation.shape)\n\n\n\nx_train = np.reshape(x_train, [-1, 28, 28, 1])\nprint(x_train.shape)\n\nx_validation = np.reshape(x_validation, [-1, 28, 28, 1])\nprint(x_validation.shape)\n\n\n\n#x_train = x_train.reshape(44800, 784)\n#x_validation = x_validation.reshape(11200, 784)\n#x_train= x_train.astype('float32')\n#x_validation = x_validation.astype('float32')\n#x_test = x_test.reshape(28000, 784)\n\n\n\n# normalize data for trainining-purpose \n\n\n\nprint('x shape')\nprint(x_train.shape)\nprint(x_validation.shape)\nprint(x_test.shape)\nprint(\"-\"*100)\nprint(\"y_train unique values unpack\")\nprint(np.unique(y_train, return_counts = True))\nprint(\"y_validation unique values unpack\")\nprint(np.unique(y_validation, return_counts = True))\nprint(\"-\"*100)\n\n\n# One hot encoding\nprint('y shape')\nnum_class = 10 \nprint('before one hot encoding:', y_train.shape)\nprint('one hot encoding in process......................')\ny_train = np_utils.to_categorical(y_train)\ny_validation = np_utils.to_categorical(y_validation)\n\nprint(y_train.shape)\nprint('after one hot ecndoing', y_train.shape)\n\n\nprint(x_train.shape)\nprint(x_validation.shape)\nprint(y_train.shape)\nprint(y_validation.shape)\n\nx_train= x_train.astype('float32')\nx_validation = x_validation.astype('float32')\nx_test = x_test.astype('float32')\n\n#x_train /= 255 \n#x_validation /= 255\n#x_test /= 255 \n\n\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape = (28,28,1)))\nmodel.add(Dense(512))\nmodel.add(Activation('tanh'))\n    \nmodel.add(Dense(256))\nmodel.add(Activation('tanh'))\n    \nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n    \nmodel = model \n\n    \n# Compile model \n\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = opt)\n\n\n#### saving metrics in history\nhistory = model.fit(x_train, y_train, batch_size = 32, epochs= 50, verbose = 2)\n\n# model save \n\nsave_dir = \"./\"\nmodel_name = 'h1_result_RMSprop_augmented'\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)","metadata":{"execution":{"iopub.status.busy":"2022-10-11T03:22:02.900094Z","iopub.execute_input":"2022-10-11T03:22:02.900417Z","iopub.status.idle":"2022-10-11T03:24:38.287129Z","shell.execute_reply.started":"2022-10-11T03:22:02.900336Z","shell.execute_reply":"2022-10-11T03:24:38.286364Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"(44800, 28, 28)\n(11200, 28, 28)\n(44800,)\n(11200,)\n(44800, 28, 28, 1)\n(11200, 28, 28, 1)\nx shape\n(44800, 28, 28, 1)\n(11200, 28, 28, 1)\n(28000, 28, 28)\n----------------------------------------------------------------------------------------------------\ny_train unique values unpack\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([4458, 5096, 4483, 4574, 4350, 4024, 4340, 4724, 4350, 4401]))\ny_validation unique values unpack\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([1128, 1239, 1068, 1129, 1069, 1037, 1126, 1158, 1127, 1119]))\n----------------------------------------------------------------------------------------------------\ny shape\nbefore one hot encoding: (44800,)\none hot encoding in process......................\n(44800, 10)\nafter one hot ecndoing (44800, 10)\n(44800, 28, 28, 1)\n(11200, 28, 28, 1)\n(44800, 10)\n(11200, 10)\n","output_type":"stream"},{"name":"stderr","text":"2022-10-11 03:22:11.727484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:11.823608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:11.824500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:11.826122: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-10-11 03:22:11.827353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:11.828059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:11.828683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:14.087997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:14.088863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:14.089538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-10-11 03:22:14.090134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2022-10-11 03:22:15.090987: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n1400/1400 - 4s - loss: 0.4639 - accuracy: 0.8627\nEpoch 2/50\n1400/1400 - 3s - loss: 0.4828 - accuracy: 0.8576\nEpoch 3/50\n1400/1400 - 2s - loss: 0.5083 - accuracy: 0.8558\nEpoch 4/50\n1400/1400 - 2s - loss: 0.5121 - accuracy: 0.8538\nEpoch 5/50\n1400/1400 - 2s - loss: 0.5310 - accuracy: 0.8499\nEpoch 6/50\n1400/1400 - 2s - loss: 0.5487 - accuracy: 0.8491\nEpoch 7/50\n1400/1400 - 2s - loss: 0.5191 - accuracy: 0.8549\nEpoch 8/50\n1400/1400 - 2s - loss: 0.5148 - accuracy: 0.8593\nEpoch 9/50\n1400/1400 - 2s - loss: 0.5245 - accuracy: 0.8565\nEpoch 10/50\n1400/1400 - 2s - loss: 0.4897 - accuracy: 0.8694\nEpoch 11/50\n1400/1400 - 2s - loss: 0.4845 - accuracy: 0.8697\nEpoch 12/50\n1400/1400 - 2s - loss: 0.4734 - accuracy: 0.8740\nEpoch 13/50\n1400/1400 - 2s - loss: 0.4422 - accuracy: 0.8849\nEpoch 14/50\n1400/1400 - 2s - loss: 0.4647 - accuracy: 0.8775\nEpoch 15/50\n1400/1400 - 2s - loss: 0.4720 - accuracy: 0.8761\nEpoch 16/50\n1400/1400 - 3s - loss: 0.4996 - accuracy: 0.8651\nEpoch 17/50\n1400/1400 - 2s - loss: 0.4876 - accuracy: 0.8665\nEpoch 18/50\n1400/1400 - 2s - loss: 0.4747 - accuracy: 0.8760\nEpoch 19/50\n1400/1400 - 2s - loss: 0.4824 - accuracy: 0.8738\nEpoch 20/50\n1400/1400 - 2s - loss: 0.4510 - accuracy: 0.8842\nEpoch 21/50\n1400/1400 - 2s - loss: 0.4452 - accuracy: 0.8840\nEpoch 22/50\n1400/1400 - 2s - loss: 0.4455 - accuracy: 0.8831\nEpoch 23/50\n1400/1400 - 2s - loss: 0.4530 - accuracy: 0.8815\nEpoch 24/50\n1400/1400 - 2s - loss: 0.4552 - accuracy: 0.8802\nEpoch 25/50\n1400/1400 - 2s - loss: 0.4604 - accuracy: 0.8805\nEpoch 26/50\n1400/1400 - 2s - loss: 0.4146 - accuracy: 0.8910\nEpoch 27/50\n1400/1400 - 2s - loss: 0.4401 - accuracy: 0.8858\nEpoch 28/50\n1400/1400 - 2s - loss: 0.4284 - accuracy: 0.8893\nEpoch 29/50\n1400/1400 - 2s - loss: 0.4124 - accuracy: 0.8961\nEpoch 30/50\n1400/1400 - 2s - loss: 0.4207 - accuracy: 0.8913\nEpoch 31/50\n1400/1400 - 3s - loss: 0.4426 - accuracy: 0.8890\nEpoch 32/50\n1400/1400 - 2s - loss: 0.4306 - accuracy: 0.8880\nEpoch 33/50\n1400/1400 - 2s - loss: 0.4367 - accuracy: 0.8867\nEpoch 34/50\n1400/1400 - 2s - loss: 0.4447 - accuracy: 0.8874\nEpoch 35/50\n1400/1400 - 2s - loss: 0.4155 - accuracy: 0.8945\nEpoch 36/50\n1400/1400 - 2s - loss: 0.4099 - accuracy: 0.8952\nEpoch 37/50\n1400/1400 - 2s - loss: 0.4328 - accuracy: 0.8882\nEpoch 38/50\n1400/1400 - 2s - loss: 0.3941 - accuracy: 0.8989\nEpoch 39/50\n1400/1400 - 2s - loss: 0.4107 - accuracy: 0.8961\nEpoch 40/50\n1400/1400 - 2s - loss: 0.4111 - accuracy: 0.8942\nEpoch 41/50\n1400/1400 - 2s - loss: 0.4038 - accuracy: 0.8948\nEpoch 42/50\n1400/1400 - 2s - loss: 0.4044 - accuracy: 0.8956\nEpoch 43/50\n1400/1400 - 2s - loss: 0.4058 - accuracy: 0.8944\nEpoch 44/50\n1400/1400 - 2s - loss: 0.4195 - accuracy: 0.8919\nEpoch 45/50\n1400/1400 - 2s - loss: 0.4388 - accuracy: 0.8856\nEpoch 46/50\n1400/1400 - 3s - loss: 0.4087 - accuracy: 0.8968\nEpoch 47/50\n1400/1400 - 2s - loss: 0.4002 - accuracy: 0.8974\nEpoch 48/50\n1400/1400 - 2s - loss: 0.4265 - accuracy: 0.8969\nEpoch 49/50\n1400/1400 - 2s - loss: 0.3815 - accuracy: 0.9068\nEpoch 50/50\n1400/1400 - 2s - loss: 0.4128 - accuracy: 0.8938\n","output_type":"stream"},{"name":"stderr","text":"2022-10-11 03:24:37.685618: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","output_type":"stream"},{"name":"stdout","text":"Saved trained model at ./h1_result_RMSprop_augmented \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With Augmentation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport math\nimport random\nimport os\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom keras.models import Sequential, load_model\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.utils import np_utils\n\nx_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.train.npy')\nsubmission = pd.read_csv('/kaggle/input/cisc6000-deep-learning-hw1/sample_submission.csv')\ny_train = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.trainlabel.npy')\nx_test = np.load('/kaggle/input/cisc6000-deep-learning-hw1/mnist.test.npy')\n\ndef seed_everything(SEED): \n    np.random.seed(SEED) \n    random.seed(SEED)\n\nseed_everything(2425)\n\nx_train, x_validation, y_train, y_validation = train_test_split(x_train,y_train,test_size = 0.2)\n\nprint(x_train.shape)\nprint(x_validation.shape)\nprint(y_train.shape)\nprint(y_validation.shape)\n\n\n\nx_train = np.reshape(x_train, [-1, 28, 28, 1])\nprint(x_train.shape)\n\nx_validation = np.reshape(x_validation, [-1, 28, 28, 1])\nprint(x_validation.shape)\n\n\n\n#x_train = x_train.reshape(44800, 784)\n#x_validation = x_validation.reshape(11200, 784)\n#x_train= x_train.astype('float32')\n#x_validation = x_validation.astype('float32')\n#x_test = x_test.reshape(28000, 784)\n\n\n\n# normalize data for trainining-purpose \n\n\n\nprint('x shape')\nprint(x_train.shape)\nprint(x_validation.shape)\nprint(x_test.shape)\nprint(\"-\"*100)\nprint(\"y_train unique values unpack\")\nprint(np.unique(y_train, return_counts = True))\nprint(\"y_validation unique values unpack\")\nprint(np.unique(y_validation, return_counts = True))\nprint(\"-\"*100)\n\n\n# One hot encoding\nprint('y shape')\nnum_class = 10 \nprint('before one hot encoding:', y_train.shape)\nprint('one hot encoding in process......................')\ny_train = np_utils.to_categorical(y_train)\ny_validation = np_utils.to_categorical(y_validation)\n\nprint(y_train.shape)\nprint('after one hot ecndoing', y_train.shape)\n\n\nprint(x_train.shape)\nprint(x_validation.shape)\nprint(y_train.shape)\nprint(y_validation.shape)\n\nx_train= x_train.astype('float32')\nx_validation = x_validation.astype('float32')\nx_test = x_test.astype('float32')\n\n#x_train /= 255 \n#x_validation /= 255\n#x_test /= 255 \n\n\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape = (28,28,1)))\nmodel.add(Dense(512))\nmodel.add(Activation('tanh'))\n    \nmodel.add(Dense(256))\nmodel.add(Activation('tanh'))\n    \nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n    \nmodel = model \n\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = opt)\n\n\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator()\n\n\ndatagen = ImageDataGenerator(        \n        rotation_range = 45,\n        horizontal_flip = True,vertical_flip = True \n        )\ndatagen.fit(x_train)\n\n\n\ntrain_generator = datagen.flow(x_train, y_train, batch_size=32, shuffle=True)\nvalidation_generator = datagen.flow(x_validation, y_validation, batch_size=32, shuffle=True)\n\n\n\n\nstart = time.time()\n\n\nhistory_inception_datagen = model.fit_generator(train_generator,\n                                                steps_per_epoch = 1400,\n                                                epochs=50,\n                                                validation_data = validation_generator\n                                                )\n\n\n\nelapsed_time_fl = (time.time() - start)    \nprint(elapsed_time_fl)\n\n\nsave_dir = \"./\"\nmodel_name = 'h1_result_RMSprop_augmented'\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Overall validation accuracy & Loss ","metadata":{}},{"cell_type":"code","source":"y_validation = np_utils.to_categorical(y_validation)\n\nmnist_model = load_model(model_name)\nloss_and_metrics = mnist_model.evaluate(x_validation, y_validation, verbose = 2)\n\nprint(\"val loss\", loss_and_metrics[0])\nprint(\"val accuracy\", loss_and_metrics[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pred_test","metadata":{}},{"cell_type":"code","source":"# Each class' accuracy\n\n\nmnist_model = load_model(model_name)\ny_pred = np.argmax(mnist_model.predict(x_validation), axis = 1)\n\nnum_correct_class = [0 for i in range(10)]\nnum_sample_class = [0 for i in range(10)]\n\n\nfor i in range(len(y_pred)):\n    label = y_validation[i]\n    if y_pred[i] == label:\n        num_correct_class[label] +=1\n        \n    num_sample_class[label]+=1 \n\n    \nfor i in range(10):\n    acc = 100 * num_correct_class[i]/ num_sample_class[i]\n    print(f' accuray of class {i} = {acc:.2f}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data accuracy\n\npred_test = np.argmax(mnist_model.predict(x_test), axis = 1)\nsubmission = pd.read_csv('/kaggle/input/cisc6000-deep-learning-hw1/sample_submission.csv',index_col = 0)\nsubmission['class'] = pred_test\nsubmission.to_csv('RMSprop_augmented-Neural_Network_Submission.csv')\nsubmission.head()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_validation.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_validation.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_validation.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shape = (y_train.size, y_train.max()+1)\none_hot = np.zeros(shape)\n\nrows = np.arange(y_train.size)\none_hot[rows, y_train] = 1\ny_train = one_hot\n\n\n\nshape = (y_validation.size, y_validation.max()+1)\none_hot = np.zeros(shape)\n\nrows = np.arange(y_validation.size)\none_hot[rows, y_validation] = 1\ny_validation = one_hot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}